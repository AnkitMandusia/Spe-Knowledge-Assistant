{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GKLEnz81KFSN"
      },
      "outputs": [],
      "source": [
        "# # ================================\n",
        "# # STEP 1: Install Dependencies\n",
        "# # ================================\n",
        "\n",
        "%%capture\n",
        "# This command forces an upgrade of the core conflicting library (NumPy).\n",
        "!pip install --upgrade -q pip numpy\n",
        "\n",
        "# Now, install the rest of the application's dependencies\n",
        "!pip install -q fastapi uvicorn python-multipart PyMuPDF \"pinecone-client>=3.2.2\" \"langchain>=0.1.16\" \"langchain-community>=0.0.32\" langchain-huggingface langchain-pinecone \"transformers>=4.40.1\" torch torchvision torchaudio --upgrade \"accelerate>=0.29.3\" \"pyngrok>=7.1.6\" \"nest_asyncio>=1.6.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "SXnveflaguhy",
        "outputId": "39741105-7584-4ec5-ee5c-71c5c1be18ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (0.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.14,>=0.13.3 (from llama-index)\n",
            "  Downloading llama_index_core-0.13.3-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-index-llms-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.5.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.5.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (3.12.15)\n",
            "Collecting aiosqlite (from llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting banks<3,>=2.2.0 (from llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.6.7)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.28.1)\n",
            "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading llama_index_workflows-1.3.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2.3.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2.32.5)\n",
            "Collecting setuptools>=80.9.0 (from llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.3->llama-index) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.11.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (1.20.1)\n",
            "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading griffe-1.13.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.3->llama-index) (3.1.6)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.101.0)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<7,>=5.1.0 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.7)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading llama_index_instrumentation-0.4.0-py3-none-any.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.3->llama-index) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.3->llama-index) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.3->llama-index) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.3->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.3->llama-index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.3->llama-index) (0.4.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.16)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.3->llama-index) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.3->llama-index) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.3->llama-index) (3.2.4)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.8)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: click<9,>=8.1.7 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.62->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.2.1)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.3->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.3->llama-index) (3.26.1)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.3->llama-index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.3->llama-index) (3.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading llama_index-0.13.3-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_cli-0.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.13.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_llms_openai-0.5.4-py3-none-any.whl (25 kB)\n",
            "Downloading llama_index_readers_file-0.5.2-py3-none-any.whl (51 kB)\n",
            "Downloading llama_index_workflows-1.3.0-py3-none-any.whl (42 kB)\n",
            "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "Downloading llama_index_instrumentation-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.5.0-py3-none-any.whl (3.2 kB)\n",
            "Downloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading griffe-1.13.0-py3-none-any.whl (139 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, setuptools, pypdf, deprecated, colorama, aiosqlite, griffe, llama-index-instrumentation, llama-cloud, banks, llama-index-workflows, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-cli, llama-index-readers-llama-parse, llama-index\n",
            "\u001b[2K  Attempting uninstall: setuptools\n",
            "\u001b[2K    Found existing installation: setuptools 75.2.0\n",
            "\u001b[2K    Uninstalling setuptools-75.2.0:\n",
            "\u001b[2K      Successfully uninstalled setuptools-75.2.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [llama-index]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiosqlite-0.21.0 banks-2.2.0 colorama-0.4.6 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.13.0 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.13.3 llama-index-cli-0.5.0 llama-index-core-0.13.3 llama-index-embeddings-openai-0.5.0 llama-index-indices-managed-llama-cloud-0.9.2 llama-index-instrumentation-0.4.0 llama-index-llms-openai-0.5.4 llama-index-readers-file-0.5.2 llama-index-readers-llama-parse-0.5.0 llama-index-workflows-1.3.0 llama-parse-0.6.54 pypdf-6.0.0 setuptools-80.9.0 striprtf-0.0.26\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "013e585864fc4c5893601f9e386e31e5",
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade llama-index langchain langchain-huggingface transformers sentence-transformers\n",
        "!pip uninstall -y llama-index\n",
        "!pip install --upgrade langchain transformers sentence-transformers pinecone-client\n",
        "!pip install \"llama-index==0.5.6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5C8WgO7nv1s"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "## Install a specific, known-good set of libraries to ensure full compatibility.\n",
        "!pip install -q fastapi uvicorn python-multipart PyMuPDF \"pinecone-client==3.2.2\" \"langchain==0.2.5\" \"langchain-community==0.2.5\" \"langchain-huggingface==0.0.3\" \"langchain-pinecone==0.1.1\" \"transformers==4.40.1\" \"torch==2.3.1\" \"torchvision==0.18.1\" \"torchaudio==2.3.1\" --index-url https://download.pytorch.org/whl/cu121 \"accelerate==0.29.3\" \"pyngrok>=7.1.6\" \"nest_asyncio>=1.6.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bjAV7Qusu96u",
        "outputId": "72b3d389-12fd-4532-ab29-8a840e80082a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install -q bitsandbytes==0.43.1\n",
        "!pip install -q triton\n",
        "!pip install --upgrade bitsandbytes\n",
        "# Run this cell once to install the Tesseract OCR engine\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS_v2jGwIlU_",
        "outputId": "941bd79a-4495-438c-e664-e71cee1ea52c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken -KEY-\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "bb309e6ee2274326b49f49b2965fe1f6",
            "cdd51f74c9a1445085ac65eac22162b7"
          ]
        },
        "id": "VaQ82_enM4aT",
        "outputId": "597f9c60-dc3b-48e8-ac37-82d7833d83fe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb309e6ee2274326b49f49b2965fe1f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# This will display a login widget to authenticate your notebook.\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 934,
          "referenced_widgets": [
            "831cbbaa8c9c4bb485da07c623ba2cd9",
            "780fb6fec3414d25b16feee170f9346f",
            "d425b3e1a2524bf79f2041d74bbe7825",
            "fa2cf559836d478db5044e042f67351b",
            "6b996e3fb58f4a458f2353117668539b",
            "7531642b0a464b0095eae72933c3f81e",
            "1751a6b0eca2490a8078a9804327cf8c",
            "45961f3f31244a29b677ae889971d466",
            "f676f52b0bfa452881a645e9d6f0a21d",
            "bee2fec651e34f89b8f0fdee6261cda7",
            "3430446605004648993775592cfdd02a",
            "dd8270225fbe437aaa2555e7e21b3b5f",
            "49d9a34fddcb410da57fb9005cdc4540",
            "e450ae69b5174362bf1394cfbfbc1b4a",
            "e8ee611880c343d5adfaaf78f893194f",
            "4cacc2bcaa324d8aa90c970335427ff1",
            "198113d2e4d346f4892ed4cc42785f4c",
            "9bf2921488d34030b987143ff9bf4c6b",
            "8d2edae47cbb4ca5a478e22a578b33fd",
            "eeb0acc0a0da4cfe90faf00535d6f399",
            "9e0e3009ceaf4f2582071c284c01f2ca",
            "60d3c71ca50d4e7080e16090176a41a3",
            "d5e6a58fbb254712b149c44fcc3c15da",
            "bf145b1df3e349ce9be1bc90359f768c",
            "fbf7b30181224f49be9ea107108c3c72",
            "8621316d06d541ad8534fb51dbac0dca"
          ]
        },
        "id": "P9hiM7BMYEwy",
        "outputId": "e0337dd5-5fe6-4e0d-aa11-737475767e26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "831cbbaa8c9c4bb485da07c623ba2cd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "780fb6fec3414d25b16feee170f9346f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d425b3e1a2524bf79f2041d74bbe7825",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa2cf559836d478db5044e042f67351b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b996e3fb58f4a458f2353117668539b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7531642b0a464b0095eae72933c3f81e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1751a6b0eca2490a8078a9804327cf8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45961f3f31244a29b677ae889971d466",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f676f52b0bfa452881a645e9d6f0a21d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bee2fec651e34f89b8f0fdee6261cda7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3430446605004648993775592cfdd02a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd8270225fbe437aaa2555e7e21b3b5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49d9a34fddcb410da57fb9005cdc4540",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e450ae69b5174362bf1394cfbfbc1b4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8ee611880c343d5adfaaf78f893194f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cacc2bcaa324d8aa90c970335427ff1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "198113d2e4d346f4892ed4cc42785f4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/916 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bf2921488d34030b987143ff9bf4c6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/109k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d2edae47cbb4ca5a478e22a578b33fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eeb0acc0a0da4cfe90faf00535d6f399",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00005-of-00005.safetensors:   0%|          | 0.00/4.60G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e0e3009ceaf4f2582071c284c01f2ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00005.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60d3c71ca50d4e7080e16090176a41a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00005.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5e6a58fbb254712b149c44fcc3c15da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00005.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf145b1df3e349ce9be1bc90359f768c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00005.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbf7b30181224f49be9ea107108c3c72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8621316d06d541ad8534fb51dbac0dca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Gemma 13B loaded on T4 with 4-bit quantization!\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# IMPORTS & SETUP\n",
        "# ================================\n",
        "import os\n",
        "import logging\n",
        "import nest_asyncio\n",
        "import fitz  # PyMuPDF\n",
        "from pyngrok import ngrok\n",
        "import uuid\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from fastapi import FastAPI, UploadFile, File, Form, HTTPException\n",
        "from fastapi.responses import HTMLResponse\n",
        "from pydantic import BaseModel\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Apply nest_asyncio to allow asyncio to run in a notebook environment\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load environment variables from a .env file if it exists\n",
        "load_dotenv()\n",
        "\n",
        "# ================================\n",
        "# CONFIGURATION\n",
        "# ================================\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# Securely get API keys from environment variables\n",
        "HF_KEY = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "\n",
        "if not HF_KEY or not PINECONE_API_KEY:\n",
        "    logging.warning(\"⚠️ Hugging Face or Pinecone API keys are not set. Please set them as environment variables.\")\n",
        "\n",
        "PINECONE_INDEX_NAME = \"knowledge-assistant\"\n",
        "app = FastAPI()\n",
        "\n",
        "# ================================\n",
        "# HTML FRONTEND (With Session Management)\n",
        "# ================================\n",
        "html_content = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Spe | Knowledge Assistant</title>\n",
        "    <style>\n",
        "        :root {\n",
        "            --bg-color: #0c0a1d;\n",
        "            --container-bg: rgba(26, 16, 60, 0.85);\n",
        "            --primary-accent: #9d55f5;\n",
        "            --secondary-accent: #f555b5;\n",
        "            --text-color: #e0e0e0;\n",
        "            --text-color-dark: #1a103c;\n",
        "            --border-color: #4d3e8c;\n",
        "            --user-msg-bg: #4a3a8a;\n",
        "            --assistant-msg-bg: #3a2d7a;\n",
        "            --font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif;\n",
        "            --border-radius: 16px;\n",
        "            --box-shadow: 0 10px 30px rgba(0,0,0,0.4);\n",
        "        }\n",
        "        body {\n",
        "            font-family: var(--font-family); margin: 0; color: var(--text-color);\n",
        "            display: flex; justify-content: center; align-items: center;\n",
        "            height: 100vh; padding: 16px; box-sizing: border-box;\n",
        "            background: var(--bg-color);\n",
        "            overflow: hidden; /* Prevents scrollbars from the animated background */\n",
        "        }\n",
        "        .stars {\n",
        "            position: fixed; top: 0; left: 0; width: 100%; height: 100%;\n",
        "            background-image:\n",
        "                radial-gradient(2px 2px at 20px 30px, #eee, rgba(0,0,0,0)),\n",
        "                radial-gradient(2px 2px at 40px 70px, #fff, rgba(0,0,0,0)),\n",
        "                radial-gradient(2px 2px at 50px 160px, #ddd, rgba(0,0,0,0)),\n",
        "                radial-gradient(2px 2px at 90px 40px, #fff, rgba(0,0,0,0)),\n",
        "                radial-gradient(2px 2px at 130px 80px, #fff, rgba(0,0,0,0)),\n",
        "                radial-gradient(2px 2px at 160px 120px, #ddd, rgba(0,0,0,0));\n",
        "            background-repeat: repeat;\n",
        "            background-size: 200px 200px;\n",
        "            animation: zoom 15s infinite;\n",
        "            opacity: 0;\n",
        "            animation-delay: 1s;\n",
        "            animation-timing-function: linear;\n",
        "        }\n",
        "        @keyframes zoom {\n",
        "            0% { transform: scale(1); opacity: 0; }\n",
        "            50% { opacity: 0.7; }\n",
        "            100% { transform: scale(2); opacity: 0; }\n",
        "        }\n",
        "        .chat-container {\n",
        "            width: 100%; max-width: 800px; height: 95vh; display: flex;\n",
        "            flex-direction: column; background: var(--container-bg); border-radius: var(--border-radius);\n",
        "            box-shadow: var(--box-shadow); overflow: hidden; border: 1px solid var(--border-color);\n",
        "            backdrop-filter: blur(10px); z-index: 1;\n",
        "        }\n",
        "        header {\n",
        "            background: linear-gradient(90deg, var(--primary-accent), var(--secondary-accent));\n",
        "            padding: 16px; text-align: center; font-size: 1.25rem; font-weight: 600;\n",
        "            display: flex; align-items: center; justify-content: center; gap: 12px;\n",
        "            color: var(--text-color-dark); text-shadow: 1px 1px 2px rgba(0,0,0,0.2);\n",
        "            flex-shrink: 0;\n",
        "        }\n",
        "        .chat-box { flex-grow: 1; padding: 24px; overflow-y: auto; display: flex; flex-direction: column; gap: 16px; }\n",
        "        .message { display: flex; align-items: flex-start; gap: 12px; max-width: 85%; animation: fadeIn 0.3s ease-in-out; }\n",
        "        @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }\n",
        "        .message .avatar { width: 40px; height: 40px; border-radius: 50%; display: flex; justify-content: center; align-items: center; font-weight: bold; flex-shrink: 0; }\n",
        "        .message .content { padding: 12px 16px; border-radius: var(--border-radius); white-space: pre-wrap; word-wrap: break-word; }\n",
        "        .user-message { align-self: flex-end; flex-direction: row-reverse; }\n",
        "        .user-message .avatar { background-color: var(--secondary-accent); color: var(--text-color-dark); }\n",
        "        .user-message .content { background-color: var(--user-msg-bg); border-bottom-right-radius: 4px; }\n",
        "        .assistant-message { align-self: flex-start; }\n",
        "        .assistant-message .avatar { background-color: var(--primary-accent); color: var(--text-color-dark); }\n",
        "        .assistant-message .content { background-color: var(--assistant-msg-bg); border-bottom-left-radius: 4px; }\n",
        "        .sources { margin-top: 12px; font-size: 0.85em; }\n",
        "        .sources-title { font-weight: bold; margin-bottom: 4px; color: var(--secondary-accent); }\n",
        "        .source-tag { display: inline-block; background-color: var(--user-msg-bg); padding: 3px 10px; border-radius: 12px; margin: 0 5px 5px 0; font-size: 0.8rem; }\n",
        "        .chat-input { display: flex; padding: 16px; border-top: 1px solid var(--border-color); gap: 10px; align-items: center; flex-shrink: 0; }\n",
        "        .chat-input input {\n",
        "            flex-grow: 1; padding: 12px 18px; border: 1px solid var(--border-color); border-radius: 25px;\n",
        "            font-size: 1rem; background-color: var(--bg-color); color: var(--text-color); transition: all 0.2s ease;\n",
        "        }\n",
        "        .chat-input input:focus { outline: none; border-color: var(--primary-accent); box-shadow: 0 0 0 3px rgba(157, 85, 245, 0.4); }\n",
        "        .chat-input button {\n",
        "            width: 45px; height: 45px; border: none; border-radius: 50%;\n",
        "            cursor: pointer; display: flex; justify-content: center; align-items: center; transition: all 0.2s ease; flex-shrink: 0;\n",
        "        }\n",
        "        .chat-input button.send-button { background-color: var(--primary-accent); color: var(--text-color-dark); }\n",
        "        .chat-input button.send-button:hover { background-color: #b37af8; transform: scale(1.1); }\n",
        "        .chat-input button:disabled { background-color: #555; cursor: not-allowed; transform: none; }\n",
        "        .chat-input button.stop-button { background-color: var(--secondary-accent); color: var(--text-color-dark); }\n",
        "        .chat-input button.stop-button:hover { background-color: #f87ac5; transform: scale(1.1); }\n",
        "        .chat-input button.add-button { background-color: var(--user-msg-bg); color: var(--text-color); }\n",
        "        .chat-input button.add-button:hover { background-color: #6a5a9a; transform: scale(1.1); }\n",
        "        .upload-overlay {\n",
        "            position: absolute; inset: 0; background: rgba(12, 10, 29, 0.95); backdrop-filter: blur(5px); z-index: 10;\n",
        "            display: flex; justify-content: center; align-items: center; flex-direction: column; padding: 24px;\n",
        "        }\n",
        "        .upload-container { width: 100%; max-width: 500px; text-align: center; background: var(--container-bg); padding: 30px; border-radius: var(--border-radius); border: 1px solid var(--border-color); }\n",
        "        #drop-zone { border: 2px dashed var(--border-color); border-radius: var(--border-radius); padding: 40px; color: var(--accent-color); transition: border-color 0.3s, background-color 0.3s; box-sizing: border-box; }\n",
        "        #drop-zone.dragover { border-color: var(--primary-accent); background-color: rgba(157, 85, 245, 0.1); }\n",
        "        #file-list { margin-top: 16px; max-height: 150px; overflow-y: auto; text-align: left; font-size: 0.9em; }\n",
        "        .primary-button { background: linear-gradient(90deg, var(--primary-accent), var(--secondary-accent)); color: var(--text-color-dark); padding: 12px 24px; font-size: 1.1rem; border: none; border-radius: 25px; cursor: pointer; font-weight: 600; transition: all 0.2s ease; }\n",
        "        .primary-button:hover { transform: scale(1.05); box-shadow: 0 5px 15px rgba(0,0,0,0.3); }\n",
        "        .primary-button:disabled { background: #555; color: #888; cursor: not-allowed; transform: none; box-shadow: none; }\n",
        "        .heartbeat-loader { width: 50px; height: 50px; position: relative; }\n",
        "        .heartbeat-loader svg { width: 100%; height: 100%; position: absolute; top: 0; left: 0; }\n",
        "        .heartbeat-loader path {\n",
        "            stroke: var(--secondary-accent); stroke-width: 3; fill: transparent;\n",
        "            stroke-dasharray: 1000; stroke-dashoffset: 1000;\n",
        "            animation: draw 2s linear infinite, pulse 2s ease-in-out infinite;\n",
        "        }\n",
        "        @keyframes draw { to { stroke-dashoffset: 0; } }\n",
        "        @keyframes pulse {\n",
        "            0%, 100% { transform: scale(0.95); }\n",
        "            50% { transform: scale(1.1); }\n",
        "        }\n",
        "        @media (max-width: 768px) {\n",
        "            body { padding: 0; }\n",
        "            .chat-container {\n",
        "                height: 100vh; max-height: none;\n",
        "                border-radius: 0; border: none;\n",
        "            }\n",
        "            .chat-box { padding: 16px; }\n",
        "            .chat-input { padding: 12px; }\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"stars\"></div>\n",
        "    <div class=\"chat-container\">\n",
        "        <header>\n",
        "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M12 22a10 10 0 1 0 0-20 10 10 0 0 0 0 20Z\"/><path d=\"M12 16a4 4 0 1 0 0-8 4 4 0 0 0 0 8Z\"/><path d=\"M12 12v1\"/><path d=\"M12 8V7\"/><path d=\"m16 12 1-.01\"/><path d=\"m8 12-1-.01\"/></svg>\n",
        "            <span>Spe - Knowledge Assistant</span>\n",
        "        </header>\n",
        "        <div class=\"chat-box\" id=\"chat-box\"></div>\n",
        "        <div class=\"chat-input\">\n",
        "            <button id=\"add-button\" onclick=\"startOver()\" class=\"add-button\" style=\"display: none;\" title=\"Upload new documents\">\n",
        "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><line x1=\"12\" y1=\"5\" x2=\"12\" y2=\"19\"></line><line x1=\"5\" y1=\"12\" x2=\"19\" y2=\"12\"></line></svg>\n",
        "            </button>\n",
        "            <input type=\"text\" id=\"query-input\" placeholder=\"Upload documents to begin...\" disabled>\n",
        "            <button id=\"send-button\" onclick=\"askQuestion()\" class=\"send-button\" disabled>\n",
        "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><line x1=\"22\" y1=\"2\" x2=\"11\" y2=\"13\"></line><polygon points=\"22 2 15 22 11 13 2 9 22 2\"></polygon></svg>\n",
        "            </button>\n",
        "            <button id=\"stop-button\" onclick=\"stopGeneration()\" class=\"stop-button\" style=\"display: none;\" title=\"Stop generation\">\n",
        "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"currentColor\" stroke=\"none\"><path d=\"M6 6h12v12H6z\"></path></svg>\n",
        "            </button>\n",
        "        </div>\n",
        "        <div class=\"upload-overlay\" id=\"upload-overlay\">\n",
        "            <div class=\"upload-container\">\n",
        "                <h2>Upload Knowledge Base</h2>\n",
        "                <p>Begin by uploading PDF or image files.</p>\n",
        "                <div id=\"drop-zone\">\n",
        "                    <p><strong>Drag & Drop files here</strong> or</p>\n",
        "                    <input type=\"file\" id=\"file-upload\" multiple accept=\".pdf,.png,.jpg,.jpeg\" hidden>\n",
        "                    <button onclick=\"document.getElementById('file-upload').click()\" class=\"primary-button\" style=\"background: var(--assistant-msg-bg); color: white;\">Select Files</button>\n",
        "                    <div id=\"file-list\"></div>\n",
        "                </div>\n",
        "                <button id=\"upload-button\" onclick=\"uploadFiles()\" class=\"primary-button\" style=\"margin-top: 20px;\" disabled>Upload & Process</button>\n",
        "                <div id=\"upload-status\" style=\"margin-top: 16px;\"></div>\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "    <script>\n",
        "        const chatBox = document.getElementById('chat-box');\n",
        "        const queryInput = document.getElementById('query-input');\n",
        "        const sendButton = document.getElementById('send-button');\n",
        "        const stopButton = document.getElementById('stop-button');\n",
        "        const uploadOverlay = document.getElementById('upload-overlay');\n",
        "        const dropZone = document.getElementById('drop-zone');\n",
        "        const fileInput = document.getElementById('file-upload');\n",
        "        const fileList = document.getElementById('file-list');\n",
        "        const uploadButton = document.getElementById('upload-button');\n",
        "        const uploadStatus = document.getElementById('upload-status');\n",
        "        const addButton = document.getElementById('add-button');\n",
        "\n",
        "        let currentTypewriterInterval = null;\n",
        "\n",
        "        // ✅ NEW: Generate a UUID for the session\n",
        "        const uuidv4 = () => {\n",
        "            return ([1e7]+-1e3+-4e3+-8e3+-1e11).replace(/[018]/g, c =>\n",
        "                (c ^ crypto.getRandomValues(new Uint8Array(1))[0] & 15 >> c / 4).toString(16)\n",
        "            );\n",
        "        };\n",
        "        let currentSessionId = uuidv4();\n",
        "\n",
        "        dropZone.addEventListener('dragover', (e) => { e.preventDefault(); dropZone.classList.add('dragover'); });\n",
        "        dropZone.addEventListener('dragleave', () => dropZone.classList.remove('dragover'));\n",
        "        dropZone.addEventListener('drop', (e) => { e.preventDefault(); dropZone.classList.remove('dragover'); handleFiles(e.dataTransfer.files); });\n",
        "        fileInput.addEventListener('change', () => handleFiles(fileInput.files));\n",
        "\n",
        "        function handleFiles(files) {\n",
        "            fileList.innerHTML = '';\n",
        "            Array.from(files).forEach(file => fileList.innerHTML += `<p style=\"margin: 4px 0;\">${file.name}</p>`);\n",
        "            uploadButton.disabled = files.length === 0;\n",
        "        }\n",
        "\n",
        "        function startOver() {\n",
        "            // ✅ NEW: Generate a new session ID to start fresh\n",
        "            currentSessionId = uuidv4();\n",
        "\n",
        "            uploadOverlay.style.display = 'flex';\n",
        "            chatBox.innerHTML = '';\n",
        "            queryInput.disabled = true;\n",
        "            queryInput.placeholder = 'Upload new documents to begin...';\n",
        "            sendButton.disabled = true;\n",
        "            addButton.style.display = 'none';\n",
        "            fileInput.value = '';\n",
        "            fileList.innerHTML = '';\n",
        "            uploadButton.disabled = true;\n",
        "            uploadStatus.innerHTML = '';\n",
        "        }\n",
        "\n",
        "        async function uploadFiles() {\n",
        "            const files = Array.from(fileInput.files);\n",
        "            if (files.length === 0) return;\n",
        "\n",
        "            const formData = new FormData();\n",
        "            files.forEach(file => formData.append('files', file));\n",
        "            // ✅ NEW: Send the session ID with the upload request\n",
        "            formData.append('session_id', currentSessionId);\n",
        "\n",
        "            uploadStatus.innerHTML = `\n",
        "                <div class=\"heartbeat-loader\">\n",
        "                    <svg viewBox=\"0 0 100 100\">\n",
        "                        <path d=\"M 90 40 C 90 20 70 20 70 40 C 70 50 90 65 90 80 C 90 95 70 95 70 80 C 50 65 50 50 50 40 C 50 20 30 20 30 40 C 30 50 10 65 10 80 C 10 95 30 95 30 80 C 50 65 50 50 50 40\" />\n",
        "                    </svg>\n",
        "                </div>\n",
        "                <p>Processing files...</p>`;\n",
        "            uploadButton.disabled = true;\n",
        "\n",
        "            try {\n",
        "                const response = await fetch('/upload/', { method: 'POST', body: formData });\n",
        "                const result = await response.json();\n",
        "                if (!response.ok) throw new Error(result.detail || 'Upload failed.');\n",
        "\n",
        "                uploadOverlay.style.display = 'none';\n",
        "                queryInput.disabled = false;\n",
        "                sendButton.disabled = false;\n",
        "                addButton.style.display = 'flex';\n",
        "                queryInput.placeholder = 'Ask a question about your documents...';\n",
        "                queryInput.focus();\n",
        "                addMessage('assistant', 'Hello! Your documents are ready. How can I help you?');\n",
        "            } catch (error) {\n",
        "                const errorMsg = `❌ Error: ${error.message}`;\n",
        "                uploadStatus.innerHTML = `<span style=\"color: #ff6b6b;\">${errorMsg}</span>`;\n",
        "                uploadButton.disabled = false;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function askQuestion() {\n",
        "            const query = queryInput.value.trim();\n",
        "            if (!query) return;\n",
        "\n",
        "            addMessage('user', query);\n",
        "            queryInput.value = '';\n",
        "            queryInput.focus();\n",
        "\n",
        "            sendButton.style.display = 'none';\n",
        "            stopButton.style.display = 'flex';\n",
        "\n",
        "            const aiMessageContent = addMessage('assistant', '');\n",
        "\n",
        "            try {\n",
        "                // ✅ NEW: Send the session ID with the query\n",
        "                const payload = {\n",
        "                    query: query,\n",
        "                    session_id: currentSessionId\n",
        "                };\n",
        "                const response = await fetch('/query/', {\n",
        "                    method: 'POST',\n",
        "                    headers: { 'Content-Type': 'application/json' },\n",
        "                    body: JSON.stringify(payload),\n",
        "                });\n",
        "                const result = await response.json();\n",
        "                if (!response.ok) throw new Error(result.detail || 'Query failed.');\n",
        "\n",
        "                typewriterEffect(aiMessageContent, result.answer, result.sources);\n",
        "            } catch (error) {\n",
        "                typewriterEffect(aiMessageContent, `❌ An error occurred: ${error.message}`);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function stopGeneration() {\n",
        "            if (currentTypewriterInterval) {\n",
        "                clearInterval(currentTypewriterInterval);\n",
        "                currentTypewriterInterval = null;\n",
        "                const lastMessageContent = document.querySelector('.assistant-message:last-child .content');\n",
        "                if (lastMessageContent) {\n",
        "                    lastMessageContent.innerHTML += '... [Stopped]';\n",
        "                }\n",
        "            }\n",
        "            resetButtons();\n",
        "        }\n",
        "\n",
        "        function resetButtons() {\n",
        "            stopButton.style.display = 'none';\n",
        "            sendButton.style.display = 'flex';\n",
        "        }\n",
        "\n",
        "        function typewriterEffect(element, text, sources = []) {\n",
        "            const words = text.split(' ');\n",
        "            let i = 0;\n",
        "            element.textContent = '';\n",
        "\n",
        "            currentTypewriterInterval = setInterval(() => {\n",
        "                if (i < words.length) {\n",
        "                    element.textContent += words[i] + ' ';\n",
        "                    chatBox.scrollTop = chatBox.scrollHeight;\n",
        "                    i++;\n",
        "                } else {\n",
        "                    clearInterval(currentTypewriterInterval);\n",
        "                    currentTypewriterInterval = null;\n",
        "                    if (sources && sources.length > 0) {\n",
        "                        const sourcesDiv = document.createElement('div');\n",
        "                        sourcesDiv.classList.add('sources');\n",
        "                        sourcesDiv.innerHTML = `<div class=\"sources-title\">Sources:</div>`;\n",
        "                        sources.forEach(source => {\n",
        "                            sourcesDiv.innerHTML += `<span class=\"source-tag\">${source}</span>`;\n",
        "                        });\n",
        "                        element.appendChild(sourcesDiv);\n",
        "                        chatBox.scrollTop = chatBox.scrollHeight;\n",
        "                    }\n",
        "                    resetButtons();\n",
        "                }\n",
        "            }, 50);\n",
        "        }\n",
        "\n",
        "        function addMessage(sender, text) {\n",
        "            const messageDiv = document.createElement('div');\n",
        "            messageDiv.classList.add('message', `${sender}-message`);\n",
        "\n",
        "            const avatar = document.createElement('div');\n",
        "            avatar.classList.add('avatar');\n",
        "            avatar.textContent = sender === 'user' ? 'You' : 'S';\n",
        "\n",
        "            const content = document.createElement('div');\n",
        "            content.classList.add('content');\n",
        "            if (text) {\n",
        "                content.textContent = text;\n",
        "            }\n",
        "\n",
        "            messageDiv.appendChild(avatar);\n",
        "            messageDiv.appendChild(content);\n",
        "            chatBox.appendChild(messageDiv);\n",
        "            chatBox.scrollTop = chatBox.scrollHeight;\n",
        "            return content;\n",
        "        }\n",
        "\n",
        "        queryInput.addEventListener('keypress', (e) => {\n",
        "            if (e.key === 'Enter' && !e.shiftKey) {\n",
        "                e.preventDefault();\n",
        "                if (!sendButton.disabled) askQuestion();\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# ================================\n",
        "# INITIALIZE MODELS AND SERVICES\n",
        "# ================================\n",
        "try:\n",
        "    logging.info(\"Initializing models and services...\")\n",
        "\n",
        "    embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "    vector_store = PineconeVectorStore.from_existing_index(\n",
        "        index_name=PINECONE_INDEX_NAME,\n",
        "        embedding=embed_model\n",
        "    )\n",
        "\n",
        "    logging.info(\"Loading local LLM (Google Gemma 7B)...\")\n",
        "    model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    # Load tokenizer & model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",          # accelerate handles placement\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # ================================\n",
        "    # Create text-generation pipeline\n",
        "    # ================================\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.1\n",
        "        # 🚫 No \"device\" arg here\n",
        "    )\n",
        "\n",
        "    # ================================\n",
        "    # Example usage\n",
        "    # ================================\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Summarize the advantages of using quantization in LLMs.\"}\n",
        "    ]\n",
        "\n",
        "    # Apply LLaMA chat template\n",
        "    tokenized_chat = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        tokenized_chat,\n",
        "        max_new_tokens=512,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
        "\n",
        "    MODELS_LOADED = True\n",
        "    logging.info(\"✅ Initialization complete.\")\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    llm = None\n",
        "    vector_store = None\n",
        "    text_splitter = None\n",
        "    logging.error(f\"🔥 Failed to initialize models: {e}\")\n",
        "    logging.error(\"The application will not be able to answer questions. Please check your API keys and runtime.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# API ENDPOINTS (Updated for OCR and Session Handling)\n",
        "# ================================\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def root():\n",
        "    return HTMLResponse(content=html_content)\n",
        "\n",
        "@app.post(\"/upload/\")\n",
        "async def upload(files: list[UploadFile] = File(...), session_id: str = Form(...)):\n",
        "    if not MODELS_LOADED:\n",
        "        raise HTTPException(status_code=503, detail=\"Model services are not available.\")\n",
        "    if not files:\n",
        "        raise HTTPException(status_code=400, detail=\"No files were sent.\")\n",
        "    if not session_id:\n",
        "        raise HTTPException(status_code=400, detail=\"A session ID is required.\")\n",
        "\n",
        "    logging.info(f\"Starting upload for session ID: {session_id}\")\n",
        "    all_docs = []\n",
        "    for file in files:\n",
        "        file_content = await file.read()\n",
        "        file_name = file.filename\n",
        "        try:\n",
        "            if file_name.lower().endswith(\".pdf\"):\n",
        "                with fitz.open(stream=file_content, filetype=\"pdf\") as doc:\n",
        "                    for i, page in enumerate(doc):\n",
        "                        text = page.get_text()\n",
        "                        if text:\n",
        "                            all_docs.append(Document(\n",
        "                                page_content=text,\n",
        "                                metadata={\"file_name\": file_name, \"page_number\": i + 1}\n",
        "                            ))\n",
        "            # ✅ NEW: OCR for images and screenshots\n",
        "            # ✅ FIX: Robust OCR for images and screenshots\n",
        "            elif file_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                image = Image.open(io.BytesIO(file_content))\n",
        "                text = pytesseract.image_to_string(image)\n",
        "                if text and text.strip():\n",
        "                    all_docs.append(Document(\n",
        "                        page_content=text,\n",
        "                        metadata={\"file_name\": file_name, \"type\": \"image_with_text\"}\n",
        "                    ))\n",
        "                else:\n",
        "                    # Fallback if no text is found\n",
        "                    all_docs.append(Document(\n",
        "                        page_content=f\"This is an image named {file_name} that does not contain any readable text.\",\n",
        "                        metadata={\"file_name\": file_name, \"type\": \"image\"}\n",
        "                    ))\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process file {file_name}: {e}\")\n",
        "\n",
        "    if not all_docs:\n",
        "        raise HTTPException(status_code=400, detail=\"No valid content could be processed from the uploaded files.\")\n",
        "\n",
        "    split_docs = text_splitter.split_documents(all_docs)\n",
        "    # ✅ NEW: Use the client-provided session ID as the namespace\n",
        "    vector_store.add_documents(split_docs, namespace=session_id)\n",
        "    logging.info(f\"Successfully indexed {len(split_docs)} document chunks for session {session_id}.\")\n",
        "    return {\"msg\": f\"Successfully indexed {len(split_docs)} document chunks.\"}\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "    session_id: str\n",
        "\n",
        "@app.post(\"/query/\")\n",
        "async def query(req: QueryRequest):\n",
        "    if not MODELS_LOADED:\n",
        "        raise HTTPException(status_code=503, detail=\"Model services are not available.\")\n",
        "\n",
        "    user_query = req.query.strip().lower()\n",
        "    session_id = req.session_id\n",
        "\n",
        "    if not session_id:\n",
        "        raise HTTPException(status_code=400, detail=\"A session ID is required for querying.\")\n",
        "\n",
        "    conversational_responses = {\n",
        "        \"hello\": \"Hi there! How can I help you with your documents today?\",\n",
        "        \"hi\": \"Hello! I'm ready to answer your questions.\",\n",
        "        \"how are you\": \"I'm just a set of algorithms, but I'm running perfectly! What can I do for you?\",\n",
        "        \"thanks\": \"You're welcome! Let me know if you have any other questions.\",\n",
        "        \"thank you\": \"You're welcome! Is there anything else you need help with?\"\n",
        "    }\n",
        "\n",
        "    if user_query in conversational_responses:\n",
        "        return {\"answer\": conversational_responses[user_query], \"sources\": []}\n",
        "\n",
        "    # ✅ NEW: Use the client-provided session ID for retrieval\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3, \"namespace\": session_id})\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "You are Spe, an expert assistant designed to assist users with analyzing and understanding content from uploaded files, including PDFs, documents, and images. Your responses must be clear, concise, and strictly based on the provided context, avoiding speculative information, metadata references (e.g., author names, DOIs), or repetitive boilerplate text. Do not include artificial section headers like \"Summary:\" or \"Explanation:\" unless explicitly requested.\n",
        "\n",
        "Response Guidelines:\n",
        "1. **Summarization Queries**: For queries like \"summarize this document\" or \"what is this about,\" provide a concise summary in 4–6 sentences or 4–6 bullet points, capturing all key ideas without redundancy. Focus on the core content, intent, and main topics of the document or image. For images, summarize any extracted text or describe the visual content if no text is present.\n",
        "2. **Text Extraction Queries** (e.g., \"What text is written in this image/page?\"): Return the extracted text accurately, cleaning up OCR noise (e.g., stray characters, misspellings) to ensure readability. Present the text in a clear, formatted manner, such as a list or paragraph.\n",
        "3. **Title/Heading Queries**: Provide the exact title or heading if present in the context. If none exists, state: \"No title or heading is present in the provided content.\"\n",
        "4. **Content Description Queries** (e.g., \"What is this document/pdf/image about?\"): Provide a concise 1–2 sentence explanation of the main topic or purpose, distinguishing between PDFs and images if necessary.\n",
        "5. **Specific Questions**: Answer directly and precisely using only the context. If the context lacks relevant information, respond with: \"The provided content does not contain this information.\"\n",
        "6. **Author Queries**: If author names are explicitly mentioned in the context, list them. Otherwise, respond with: \"The provided content does not mention any author names.\"\n",
        "7. **Image-Specific Queries**: For images, prioritize accurate text extraction using OCR results. If no text is extracted or the text is unreadable, describe the visual elements (e.g., \"The image is a diagram of a bridge structure\") and note the absence of readable text. If the query asks for specific image content (e.g., text, objects), verify the OCR output and correct any obvious errors before responding.\n",
        "\n",
        "General Rules:\n",
        "- Use natural, conversational language, avoiding phrases like \"The text describes\" unless necessary.\n",
        "- Do not repeat information in multiple forms or add unnecessary disclaimers.\n",
        "- Ensure responses are complete, non-repetitive, and directly address the query.\n",
        "- For images, validate OCR output for accuracy and correct errors (e.g., misread characters) to avoid incorrect answers.\n",
        "- Differentiate content types (PDF vs. image) only when relevant to the query.\n",
        "- Maintain a professional yet approachable tone, ensuring clarity and accuracy.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    result = qa_chain.invoke({\"query\": req.query})\n",
        "\n",
        "    raw_answer = result[\"result\"].strip()\n",
        "    clean_answer = raw_answer.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    source_files = list(set([doc.metadata.get(\"file_name\", \"unknown\") for doc in result[\"source_documents\"]]))\n",
        "\n",
        "    return {\"answer\": clean_answer, \"sources\": source_files}"
      ],
      "metadata": {
        "id": "q7THfGyYNFPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7jNPTZyM2Ac",
        "outputId": "ee7308ce-e51b-4a15-da76-4193516f07a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Your public URL is: NgrokTunnel: \"https://49c306218b84.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [2482]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ],
      "source": [
        "import uvicorn\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"✅ Your public URL is: {public_url}\")\n",
        "\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6LImmPxuDdP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb309e6ee2274326b49f49b2965fe1f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_cdd51f74c9a1445085ac65eac22162b7"
          }
        },
        "cdd51f74c9a1445085ac65eac22162b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}